{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eFsVOn9G5XfP"
      },
      "source": [
        "# **Task \\#4 B**: Machine Learning MC886/MO444\n",
        "\n",
        "##**Natural Language Processing (NLP)**##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MKu4K425uHV",
        "outputId": "b24a890c-54e1-4c2e-df94-e3b80484f7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gabriel Borges Gutierrez RA 237300\n",
            "Marcelo Antunes Soares Fantini RA 108341\n",
            "Rubens de Castro Pereira RA 217146\n",
            "\n",
            "Notebook de Rubens - v01\n"
          ]
        }
      ],
      "source": [
        "print(\"Gabriel Borges Gutierrez\" + \" RA 237300\")\n",
        "print(\"Marcelo Antunes Soares Fantini\" + \" RA 108341\")\n",
        "print(\"Rubens de Castro Pereira\" + \" RA 217146\")\n",
        "print(\"\")\n",
        "print(f\"Notebook de Rubens - v01\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "69z8ovd45wMN"
      },
      "source": [
        "## Objective:\n",
        "\n",
        "There are two main objectives of this notebook: you can either fine-tunning a BERT model to sentiment analysis or fine-tunning a T5 model to perform translation English to Portuguese.\n",
        "\n",
        "**You can choose which task to perform. The BERT activity is relatively easy and should take less time compared to the T5 task. However, fine-tuning these models require time. Therefore, it is recommended to test the models on a small dataset (such as one batch), to ensure the functions are working correctly. Once confirmed, you can proceed to train the models on the entire dataset.**\n",
        "\n",
        "**If you complete both tasks, you will earn extra points.**\n",
        "**Obs: In this work, you can use scikit-learn, PyTorch and HuggingFace API.**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t03BZpP_cG6I"
      },
      "source": [
        "## **Sentiment Analisys**\n",
        "\n",
        "Sentiment analysis is a task in natural language processing that involves determining the sentiment expressed in a given text, classifying it as positive, negative, or neutral. It helps analyze people's opinions and emotions from text data, enabling businesses to understand customer feedback, monitor brand reputation, and make informed decisions.\n",
        "\n",
        "In this notebook, we will use the IMDB Dataset, which is widely used in the field of natural language processing and sentiment analysis. It comprises a large collection of movie reviews from the IMDB website, with each review labeled as either positive or negative based on the sentiment expressed in the text.\n",
        "\n",
        "![bert_model](https://drive.google.com/uc?export=view&id=1rWKk7K5-0MX8EkjPeRZTaG7byFQuD6Cx)\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art deep learning model for natural language processing (NLP). It is based on the Transformer architecture and is pre-trained on a large corpus of text data. BERT is designed to understand the context and meaning of words in a sentence by considering both the left and right context, enabling it to capture intricate language patterns. It has achieved remarkable results across various NLP tasks, including text classification, named entity recognition, question answering, and has significantly advanced the field of NLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImyuV0n0SWSd",
        "outputId": "d16f166d-939f-4920-bd60-b420fb699b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AyxqPdWsz5wk"
      },
      "source": [
        "### Download Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yzUSCtEcXkI",
        "outputId": "b7285e07-8783-494b-f5b1-c73334472d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded with success.\n",
            "CPU times: user 220 ms, sys: 35 ms, total: 255 ms\n",
            "Wall time: 39 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!wget -nc -q http://files.fast.ai/data/aclImdb.tgz\n",
        "!tar -xzf aclImdb.tgz\n",
        "!pip install datasets transformers -q\n",
        "print(f'Dataset downloaded with success.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hLsw2ZbcGYz",
        "outputId": "c091284e-78f0-46f9-cb6e-d42b03a45644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "print(f\"Libraries imported successfully.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vIncsecQ0Fmw"
      },
      "source": [
        "### Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuZnVdAZcQsw"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'bert_version': 'bert-base-uncased',\n",
        "    # There are multiple versions of BERT available at the following links:\n",
        "    # https://huggingface.co/google/bert_uncased_L-12_H-768_A-12\n",
        "    # https://huggingface.co/bert-base-uncased\n",
        "    # You can explore these links to access different versions of BERT.\n",
        "\n",
        "    'batch_size': 8,\n",
        "    'learning_rate': 1e-4,  # Choose a learning rate between 1e-4 and 1e-5\n",
        "    # The maximum length of the sentence (can be adjusted)\n",
        "    'max_length': 300,\n",
        "    # Choose a value between 1 and 5 (or alternatively, use early stopping)\n",
        "    'epochs': 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg63ZvdkcSB4",
        "outputId": "e68f9633-677a-49c5-c1a4-a43ab0ba0739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Important: Fix seeds so we can replicate results\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B97P4u170VEq"
      },
      "source": [
        "### Load data\n",
        "\n",
        "Here we are loading the data. For training, we will use 20k samples, 5k samples for validation, and 25k samples for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDneHgkOcjv1",
        "outputId": "0cdc05f0-cc70-4a5c-cd63-8e55eb7a4f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First three train samples:\n",
            "0: Input: On the surface, this movie would appear to deal with the psychological process called individuation, that is how to become a true self by embracing the so-called 'dark' side of human nature. Thus, we have the Darkling, a classic shadowy devilish creature desperately seeking the company (that is, recognition) of men, and the story revolves around the various ways in which this need is handled, more or less successfully. <br /><br />However, if we dig a little deeper, we find that what this movie is actually about is how you should relate to your car like you would to any other person: - in the opening scene, the main character (male car mechanic fallen from grace)is collecting bits and pieces from car wrecks with his daughter, when a car wreck nearly smashes the little girl. Lesson #1: Cars are persons embodied with immortal souls, and stealing from car wrecks is identical with grave robbery. The wicked have disturbed the dead and must be punished. - just after that, another character (Rubin) buys a car wreck intending to repair it and sell it as a once-lost-now-found famous race-car and is warned by the salesman. Lesson #2: Just like any other person, a car has a unique identity that cannot be altered nor replaced. In addition, there is the twist that Rubin actually sees a hidden quality in what most people would just think of as junk, but eventually that quality turns out to be a projection of Rubin's own personal greed for more profit. Lesson #3: Thou shalt never treat thy car as a means only, but always as an end in itself. - then we have the scene where the main character is introduced to Rubin and, more importantly, Rubin's car: The main character's assessment of the car's qualities is not just based on its outer appearance, but also by a thorough look inside the engine room. Lesson #4: A car is not just to be judged by its looks, it is what is inside that really counts. There is punishment in store for those who do not keep this lesson in mind, as we see in the scene where another man tries to sell Rubin a fake collector's car. This scene by the way also underlines the importance of lesson #3. <br /><br />There are numerous other examples in the movie of the 'car=person'-theme, and I am too tired now to bother citing all of them, but the point remains (and I guess this is what I'm really trying to say) that this movie is fun to watch if you have absolutely nothing else to do - or, if you're a car devotee.\n",
            "   Target: negative\n",
            "\n",
            "1: Input: This is the final episode we deserved. At the end of the last season, things were left in a 'life goes on' mood, which was hardly the wrap-up that this realistic series deserved. While not a happy show, this series was always one that made you think (a rare thing on television), and this is no exception. 'Is death justified by reasoning?' 'Are morals reflective of society, or is society shaped by the morals that are selected by the few in power?' 'What is a just death, and can it exist?' All of these questions, and more, are posed by the writers of this show every week, and this is their final thesis. Fine acting, great writing, wonderful camera-work, brilliant editing, clean direction. If you have seen the series and you missed this when it first ran, then get a hold on a copy somehow. If you never watched the series when it ran, then this will stand up on its own, but it may be heavy going trying to keep up with who all the characters are and what they are alluding to in their varied pasts. For those of us who were avid viewers of the series in the last two seasons, this is very satisfying viewing.\n",
            "   Target: positive\n",
            "\n",
            "2: Input: I am a big fan of cinema verite and saw this movie because I heard how interesting it was. I can honestly say it was very interesting indeed. The two lead actors are awesome, the film isn't ever boring, and the concept behind it (though obviously inspired by the Columbine killings and the home movies of the killers) is really interesting. There are some weaknesses, such as the final 20 minutes which really detracts from the realism seen in the first hour or so and the ending really doesn't make any sense at all. The shaky camera sometimes can be a distraction, but in cinema verite that is a given. But I still think the movie is very well done and the director Ben Coccio deserves some credit.\n",
            "   Target: positive\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "First three valid samples:\n",
            "0: Input: The only previous Gordon film I had watched was the kiddie adventure THE MAGIC SWORD (1962), though I followed this soon after with EMPIRE OF THE ANTS (1977); he seems to be best remembered, however, for his sci-fi work of the 1950s.<br /><br />Anyway, I happened upon this one in a DVD rental shop: hadn't I noticed Orson Welles' unmistakable figure on the sleeve, I probably wouldn't even have bothered with it – since I know the film under its original title, NECROMANCY! I'd seen a still from it on an old horror tome of my father's: the actor's presence in a film about diabolism seemed like a great idea which couldn't possibly miss, but the end result – particularly in this bastardized edition – is a disaster! I honestly felt sorry for Welles who looks bored and, rather than in his deep and commanding voice, he mutters the inane demonic invocations almost in whispers!! <br /><br />The plot is, basically, yet another retread of ROSEMARY'S BABY (1968): a couple is invited to a remote community under false pretenses and soon discover themselves to be surrounded by diabolists. The girl, played by Pamela Franklin, ostensibly has supernatural powers (passed on from her mother, who appears intermittently throughout to warn her – though, as delivered in an intense manner through clenched teeth, the latter's speeches end up being largely incoherent and the fount of immense hilarity every time she appears!) and is expected to revive Welles' deceased young son from the dead!! For what it's worth, Franklin – a genre regular, right down from her debut performance in THE INNOCENTS (1961) – isn't bad in her role (which requires some nudity and experiences several semi-eerie hallucinations during the course of the film); hubby Michael Ontkean, however, isn't up to the challenge of his John Cassavetes-like character. Some of the other girls look good as well – notably Lee Purcell, whose belated decision to help Franklin in escaping from town eventually proves her undoing.<br /><br />Events come to a head in an incredibly muddled climax, which sees the Satanists ultimately turning on Franklin and have her take the revived boy's place in the coffin (that's gratitude for you!). While the added scenes do stick out (the hilarious opening ceremony and other would-be erotic embellishments), the overall quality of the film would have still been poor without them; then again, this particular version is further sunk by the tacked-on electronic score – which is wholly inappropriate, and cheesy in the extreme!\n",
            "   Target: negative\n",
            "\n",
            "1: Input: This complicated western was a milestone in the career of JAMES STEWART after his return from war service, wanting to change his image by doing a western, which is largely regarded as the reason for the influx of westerns in the '50s since it's very impressive. Too bad it wasn't photographed in Technicolor.<br /><br />Stewart wins first prize for \"the gun that won the West\", but then has to spend the rest of the film trying to recover it when it's stolen. SHELLEY WINTERS is a shady gal with an unsavory reputation and STEPHEN McNALLY is the local bad boy gunman in Dodge City. WILL GEER is Wyatt Earp and DAN DURYEA is Shelley's bad boyfriend. And wouldn't you know that, it being a Universal-International film, TONY CURTIS and ROCK HUDSON (both quite unknown at the time) have bit roles.<br /><br />An interesting sequence features the first Indian attack, whereby CHARLES DRAKE reveals himself to be a coward who rides off, leaving Shelley alone in the horse-drawn wagon. He later redeems himself, but it's just one of the twists and turns that has the gun passing from one unsavory hand to another--but finally ending up with the rightful owner.<br /><br />STEPHEN McNALLY and JAMES STEWART have quite a final shootout that is almost as melodramatic (but not quite) as DUEL IN THE SUN's blazing guns finale. McNally makes the perfect villain and DAN DURYEA is equally treacherous in the kind of villainous role he played throughout the '40s as a low-life gunslinger.<br /><br />Tightly constructed story is extremely well directed by Anthony Mann, and it's fun to see ROCK HUDSON (credited as Young Bull) wearing Indian war paint and TONY CURTIS as a young soldier who casts longing glances at the then slim and attractive Shelley Winters.<br /><br />Well worth viewing and definitely an above average story.\n",
            "   Target: positive\n",
            "\n",
            "2: Input: Billy Crystal normally brings the crowd to laughter, but in this movie he and all the rest of them cannot bring any smile on my face.... or perhaps just one. They call it comedy, I say it's a waste of my time.\n",
            "   Target: negative\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Train size: 20000\n",
            "Valid size: 5000\n",
            "Test  size: 25000\n"
          ]
        }
      ],
      "source": [
        "max_valid = 5000\n",
        "\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print('\\nFirst three train samples:')\n",
        "for i, (source, target) in enumerate(zip(x_train[:3], y_train[:3])):\n",
        "    print(f\"{i}: Input: {source}\\n   Target: {'positive' if target else 'negative'}\\n\")\n",
        "\n",
        "print('-'*200)\n",
        "print('\\nFirst three valid samples:')\n",
        "for i, (source, target) in enumerate(zip(x_valid[:3], y_valid[:3])):\n",
        "    print(f\"{i}: Input: {source}\\n   Target: {'positive' if target else 'negative'}\\n\")\n",
        "\n",
        "print('-'*200)\n",
        "print(f'Train size: {len(x_train)}')\n",
        "print(f'Valid size: {len(x_valid)}')\n",
        "print(f'Test  size: {len(x_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqkNDG3P4BCF"
      },
      "outputs": [],
      "source": [
        "# y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "# print([True])\n",
        "# print(len(x_train_pos))\n",
        "# print([True] * len(x_train_pos))\n",
        "# print(len([True] * len(x_train_pos)))\n",
        "# print()\n",
        "# print([False])\n",
        "# print(len(x_train_neg))\n",
        "# y_train\n",
        "# y_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ln1YCiFIKzEd"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "To use text as input for a deep learning model, we first need to tokenize each sentence based on a set of rules. After tokenization, each token is assigned a correlated index, creating a feature vector. This vector is then utilized by the model to train and update the weights. Here is an example demonstrating how the BERT tokenizer works:\n",
        "\n",
        "![bert_tokenizer](https://drive.google.com/uc?export=view&id=11LioDFis0JE3ghr672PEIeaAxZO42gUL)\n",
        "\n",
        "Initially, the input sentence is divided into tokens predetermined by the BERT tokenizer. Next, the BertTokenizer introduces two special tokens: CLS and SEP. CLS represents sentence start for tasks like classification, while SEP indicates sentence separation for boundary detection within a document. Additionally, to ensure sentences are of equal length, the tokenizer employ the PAD token for each input.\n",
        "\n",
        "Finally, each token is converted into a predetermined index for BERT input. This indexing enables the Bert model to train and update its weights effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT1M1O2wcnFt",
        "outputId": "30b0b2e5-f216-4d3c-d7b7-9d479b206435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class IMDBDataset created successfully \n"
          ]
        }
      ],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.labels = torch.Tensor(labels).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = {key: value[index] for key, value in self.data.items()}\n",
        "        item['labels'] = self.labels[index]\n",
        "        return item\n",
        "\n",
        "\n",
        "print(f'Class IMDBDataset created successfully ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k07NXH79coQr",
        "outputId": "fdb5caf3-e091-47e6-85bd-0162637e9cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset and loader created successfully\n",
            "params[bert_version] : bert-base-uncased\n",
            "params[batch_size]   : 8\n",
            "\n",
            "CPU times: user 1min 18s, sys: 2.53 s, total: 1min 20s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(params['bert_version'], disable_tqdm=False)\n",
        "\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(params['bert_version'], disable_tqdm=False)\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(params['bert_version'], use_auth_token=access_token,  disable_tqdm=False)\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# model = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# access_token = \"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\"\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(params['bert_version'], use_auth_token=access_token,  disable_tqdm=False)\n",
        "# \"private/model\",\n",
        "\n",
        "## TOKENIZE\n",
        "train_encodings = tokenizer(list(x_train), truncation=True, padding=True, return_tensors='pt', max_length=params['max_length'])\n",
        "valid_encodings = tokenizer(list(x_valid), truncation=True, padding=True, return_tensors='pt', max_length=params['max_length'])\n",
        "test_encodings  = tokenizer(list(x_test),  truncation=True, padding=True, return_tensors='pt', max_length=params['max_length'])\n",
        "\n",
        "## DATASET\n",
        "train_dataset = IMDBDataset(data=train_encodings, labels=y_train)\n",
        "valid_dataset = IMDBDataset(data=valid_encodings, labels=y_valid)\n",
        "test_dataset  = IMDBDataset(data=test_encodings, labels=y_test)\n",
        "\n",
        "## DATALOADER\n",
        "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=1)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=params['batch_size'], num_workers=1)\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=params['batch_size'], num_workers=1)\n",
        "\n",
        "print()\n",
        "print(f'Dataset and loader created successfully')\n",
        "version = params['bert_version']\n",
        "batch_size = params['batch_size']\n",
        "print(f'params[bert_version] : {version}')\n",
        "print(f'params[batch_size]   : {batch_size}')\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ewXMSeaDhDt",
        "outputId": "7b5d5385-0996-423d-ea3f-0d511eb6839b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SEP] 102\n",
            "[CLS] 101\n",
            "[PAD] 0\n",
            "[UNK] 100\n"
          ]
        }
      ],
      "source": [
        "# print(len(train_dataset))\n",
        "# print(len(train_loader))\n",
        "# print()\n",
        "\n",
        "# print(len(valid_dataset))\n",
        "# print(len(valid_loader))\n",
        "# print()\n",
        "\n",
        "# print(len(test_dataset))\n",
        "# print(len(test_loader))\n",
        "# print()\n",
        "\n",
        "# Some of the common BERT tokens\n",
        "# marker for ending of a sentence\n",
        "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
        "# start of each sentence, so BERT knows we’re doing classification\n",
        "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id)  # special token for padding\n",
        "# tokens not found in training set\n",
        "print(tokenizer.unk_token, tokenizer.unk_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoEm1a6eADr8",
        "outputId": "15a8fe87-8aa4-4675-e04c-414144fcfd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
            "torch.Size([8, 300])\n",
            "torch.Size([8, 300])\n",
            "torch.Size([8, 300])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "# Examples\n",
        "data = next(iter(train_loader))\n",
        "print(data.keys())\n",
        "\n",
        "print(data[\"input_ids\"].shape)\n",
        "print(data[\"token_type_ids\"].shape)\n",
        "print(data[\"attention_mask\"].shape)\n",
        "print(data[\"labels\"].shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r3oj3uw02BIN"
      },
      "source": [
        "### Useful functions\n",
        "\n",
        "**Note:** The following functions are provided as suggestions. You are free to modify and create your own functions, classes, or code. Feel free to customize!\n",
        "\n",
        "**If the batch does not fit in memory, use gradient accumulation.**\n",
        "\n",
        "**Hint 1:** Example of gradient accumalation in PyTorch: https://kozodoi.me/blog/20210219/gradient-accumulation.\n",
        "\n",
        "**Hint 2:** If preferred, you can utilize the [Trainer](https://huggingface.co/docs/transformers/training) from Hugging Face for assistance.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LeG0KvwOA4st"
      },
      "source": [
        "### From Kaggle Sentiment Analysis using BERT:\n",
        "\n",
        "https://www.kaggle.com/code/prakharrathi25/sentiment-analysis-using-bert/notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akXPUZIbcrJW"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "        metric.add_batch(predictions=predictions, references=batch['labels'])\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses), metric.compute()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_epoch(model, data_loader, device):\n",
        "    model.eval()\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "        metric.add_batch(predictions=predictions, references=batch['labels'])\n",
        "        losses.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(losses), metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQt0k0bZ6BQg"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping():\n",
        "    '''\n",
        "      Early stopping prevents overfitting by stopping the training process when\n",
        "      the model's performance on a validation set starts to worsen.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      patience  : int\n",
        "        Tolerance for no improvement.\n",
        "      min_delta : float\n",
        "        Minimum change required.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, patience=1, min_delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.best_model_wts = None\n",
        "        self.min_delta = min_delta\n",
        "\n",
        "    def __call__(self, model, val_loss):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            return False\n",
        "\n",
        "        elif score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "        return False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hPCtzdvX2Ekm"
      },
      "source": [
        "### Train the BERT model\n",
        "\n",
        "**Note:** The following functions are provided as suggestions. You are free to modify and create your own functions, classes, or code. Feel free to customize!\n",
        "\n",
        "**Hint 1:** See the [BertForSequenceClassification](https://huggingface.co/docs/transformers/v4.30.0/en/model_doc/bert#transformers.BertForSequenceClassification) documentation for more information.\n",
        "\n",
        "**Hint 2:** Instead of saving information by epoch, you can save it by step. A step corresponds to a single update of the model's weights based on a mini-batch of data, while an epoch represents a complete pass through the entire training dataset. The number of steps is determined by the batch size and the total number of training examples, whereas the number of epochs is a user-defined hyperparameter.\n",
        "\n",
        "**Hint 3:** BERT adapts very well to classification problems, so in just 3 or 4 epochs, the results are already acceptable (**BERT Base**). If the results are still not good, check the learning rate.\n",
        "\n",
        "**Hint 4:** Conduct small tests, such as using only one batch, to train and verify the functionality of the training and evaluation functions. After confirming their effectiveness, proceed to train the model with all the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912,
          "referenced_widgets": [
            "f9e7c44c556d4b2a8fa800f686c53463",
            "bf7847c9d58c412d925d8e5c611877e8",
            "8608e9ed66cd46129730cb857d163f40",
            "8ac0b82641654225ad0e4e112683916b",
            "2ec59d496226466aa0938f2ed993e5fe",
            "a05716b32ea4404ebf3008f6cfe3d8fe",
            "0b6d22e04f8746b9b6799f789eea8975",
            "7f0806e293074e42ac1a710186f3352c",
            "dfeedd5a7cce444fb6a0011f34339496",
            "26dbaf94c642474fbc897f2d4932c272",
            "d683af27bcef413d873f90c086d8c1d1"
          ]
        },
        "id": "hK94bdgRcsR0",
        "outputId": "afbd1f74-7596-4704-c1f4-ae7266aefd3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9e7c44c556d4b2a8fa800f686c53463",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1984, -0.0967],\n",
            "        [ 0.0228,  0.1180],\n",
            "        [-0.0535,  0.0761],\n",
            "        [ 0.1896, -0.0640],\n",
            "        [ 0.2562,  0.0580],\n",
            "        [-0.1529,  0.2377],\n",
            "        [ 0.1706,  0.0156],\n",
            "        [-0.0920, -0.1030]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "------------------\n",
            "tensor([[ 0.1984, -0.0967],\n",
            "        [ 0.0228,  0.1180],\n",
            "        [-0.0535,  0.0761],\n",
            "        [ 0.1896, -0.0640],\n",
            "        [ 0.2562,  0.0580],\n",
            "        [-0.1529,  0.2377],\n",
            "        [ 0.1706,  0.0156],\n",
            "        [-0.0920, -0.1030]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a088c9ae66e3>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   train_loss, train_acc = train_epoch(\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a1cb5cc5c23a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0;31m# _, preds = torch.max(torch.tensor(outputs), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (SequenceClassifierOutput, dim=int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(params['bert_version'])\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer = None # https://pytorch.org/docs/stable/optim.html\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "# scheduler = None # https://pytorch.org/docs/stable/optim.html (not mandatory)\n",
        "total_steps = len(train_loader) * params['epochs']\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "early_stopping = EarlyStopping()\n",
        "\n",
        "history = {'train_loss': [], 'valid_loss': [],\n",
        "           'train_acc': [], 'valid_acc': []}\n",
        "\n",
        "for epoch in tqdm(range(params['epochs']), desc='Training'):\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    valid_loss, valid_acc = evaluate_epoch(\n",
        "        model,\n",
        "        valid_loader,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    history['valid_loss'].append(valid_loss)\n",
        "    history['valid_acc'].append(valid_acc)\n",
        "\n",
        "    if early_stopping(model, valid_loss):\n",
        "        break\n",
        "\n",
        "print(f'Finished the training of the model.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EJxx5tJEO5l"
      },
      "outputs": [],
      "source": [
        "history"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNTdBQu5KBG"
      },
      "source": [
        "### Evaluation of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIFWp4f3SHvn"
      },
      "outputs": [],
      "source": [
        "# Save Best Model Weights and history\n",
        "path = 'drive/MyDrive'\n",
        "torch.save(early_stopping.best_model_wts, f'{path}/weights_bert.pth')\n",
        "\n",
        "np.save(f'{path}/train_loss_bert', np.array(history['train_loss']))\n",
        "np.save(f'{path}/valid_loss_bert', np.array(history['valid_loss']))\n",
        "\n",
        "np.save(f'{path}/train_acc_bert', np.array(history['train_acc']))\n",
        "np.save(f'{path}/valid_acc_bert', np.array(history['valid_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFqxSdPmSnrb"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "train_loss = np.load(f'{path}/train_loss_bert.npy')\n",
        "train_acc = np.load(f'{path}/train_acc_bert.npy')\n",
        "\n",
        "valid_loss = np.load(f'{path}/valid_loss_bert.npy')\n",
        "valid_acc = np.load(f'{path}/valid_acc_bert.npy')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "31m0wtaT5NaD"
      },
      "source": [
        "#### Plot the Train and Valid loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t15IZRbBctC9"
      },
      "outputs": [],
      "source": [
        "## --- Insert code here --- ##\n",
        "plt(history)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o-JTZche5Rms"
      },
      "source": [
        "#### Evaluate in Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DMZycFJfDNW"
      },
      "outputs": [],
      "source": [
        "## --- Insert code here --- ##\n",
        "model = BertForSequenceClassification.from_pretrained(params[\"bert_version\"])\n",
        "model.load_state_dict(early_stopping.best_model_wts).to(device)\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids,\n",
        "                    attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "    metric.add_batch(predictions=predictions, references=batch['labels'])\n",
        "\n",
        "print(f'Test accuracy: {metric.compute()}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XjLynj25LS_4"
      },
      "source": [
        "> What is your conclusions?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b6d22e04f8746b9b6799f789eea8975": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26dbaf94c642474fbc897f2d4932c272": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec59d496226466aa0938f2ed993e5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0806e293074e42ac1a710186f3352c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8608e9ed66cd46129730cb857d163f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0806e293074e42ac1a710186f3352c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfeedd5a7cce444fb6a0011f34339496",
            "value": 0
          }
        },
        "8ac0b82641654225ad0e4e112683916b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26dbaf94c642474fbc897f2d4932c272",
            "placeholder": "​",
            "style": "IPY_MODEL_d683af27bcef413d873f90c086d8c1d1",
            "value": " 0/1 [00:03&lt;?, ?it/s]"
          }
        },
        "a05716b32ea4404ebf3008f6cfe3d8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7847c9d58c412d925d8e5c611877e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05716b32ea4404ebf3008f6cfe3d8fe",
            "placeholder": "​",
            "style": "IPY_MODEL_0b6d22e04f8746b9b6799f789eea8975",
            "value": "Training:   0%"
          }
        },
        "d683af27bcef413d873f90c086d8c1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfeedd5a7cce444fb6a0011f34339496": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9e7c44c556d4b2a8fa800f686c53463": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf7847c9d58c412d925d8e5c611877e8",
              "IPY_MODEL_8608e9ed66cd46129730cb857d163f40",
              "IPY_MODEL_8ac0b82641654225ad0e4e112683916b"
            ],
            "layout": "IPY_MODEL_2ec59d496226466aa0938f2ed993e5fe"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
